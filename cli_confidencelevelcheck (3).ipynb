{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing all required libraries: Transformers, Datasets, Accelerate, LangGraph, and Torch\n"
      ],
      "metadata": {
        "id": "aVpW7ZsiRzwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers huggingface_hub --quiet\n"
      ],
      "metadata": {
        "id": "gQbqNMBeR5cY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Loading the Dataset (Post Installation)\n",
        "\n",
        "Initially, I attempted to load the dataset using `load_dataset(\"imdb\")` right after installing the required libraries.\n",
        "\n",
        "However, I noticed that no output or data was loading correctly. After analysis, I realized the error stemmed from unresolved file system paths â€” just mentioning `\"imdb\"` wasn't enough. Using the full dataset path was supposed to solve this issue and avoid unexpected directory-related bugs.\n",
        "\n",
        "Even after adjusting the paths, I discovered that the IMDb dataset itself was causing errors due to a known compatibility issue between `datasets` and `fsspec`.\n",
        "\n",
        "So I decided to switch to a similar open-access dataset that is `yelp_polarity`, which also performs binary sentiment classification. Unfortunately, it too had download issues due to the same loader bug.\n",
        "\n",
        "\n",
        "\n",
        "So I switched to Plan B:\n",
        "\n",
        "I decided to \"manually download a sentiment dataset in CSV format\" and use it directly with pandas â€” this gave me full control, avoided dependency bugs, and allowed me to continue fine-tuning the model without relying on unstable APIs.\n",
        "\n",
        "I used this approach because it aligned with the task's requirements for using an open-access classification dataset while ensuring my training pipeline remained functional and reproducible.\n"
      ],
      "metadata": {
        "id": "4s5VJs9GR8sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the specific file (file path might differ; adjust accordingly)\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"supergoose/flan_combined_yelp_polarity_reviews_0_2_0\",\n",
        "    filename=\"data/train-00000-of-00001.parquet\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "print(\"Downloaded to:\", file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkU-SNQmSA5K",
        "outputId": "5d9acb5b-1dd5-4b6e-e4f2-b44c63390ebe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded to: /root/.cache/huggingface/hub/datasets--supergoose--flan_combined_yelp_polarity_reviews_0_2_0/snapshots/1c8123a3698002300c0a46dc39824f61110a440c/data/train-00000-of-00001.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the parquet file into a DataFrame and selecting only relevant columns\n"
      ],
      "metadata": {
        "id": "aDwcduoZSSlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(file_path)\n",
        "print(df.head(), df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-RdoV2vSXqQ",
        "outputId": "4b9bd1de-c6b3-4f0e-8ec5-ccaf10b34994"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              inputs  \\\n",
            "0  input: Write a negative yelp review.\\noutput: ...   \n",
            "1  Sentiment analysis: Wasn't in the mood for a s...   \n",
            "2  Input:  What would be an example of an positiv...   \n",
            "3  Problem: I'm not a major fan of their coffee r...   \n",
            "4  Problem: Visited again today as I was famished...   \n",
            "\n",
            "                                             targets  _template_idx  \\\n",
            "0  Went to Henderson location it's ok, another sp...              6   \n",
            "1                                           positive              8   \n",
            "2  An example of an positive review: LOVE me some...              4   \n",
            "3                                           positive              9   \n",
            "4                                           negative              9   \n",
            "\n",
            "  _task_source                   _task_name _template_type  \n",
            "0     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "1     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "2     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "3     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "4     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt   (41202, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting into train and test"
      ],
      "metadata": {
        "id": "K2d31NoGVeYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLTbYPT2VhTh",
        "outputId": "35c14e5c-4f65-4f76-b41e-2b414f5e3f65"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['inputs', 'targets', '_template_idx', '_task_source', '_task_name',\n",
              "       '_template_type'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering only positive/negative samples and mapping labels to integers (0/1)\n"
      ],
      "metadata": {
        "id": "-q4BTTL7eNiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"inputs\", \"targets\"]]\n",
        "df = df[df[\"targets\"].isin([\"positive\", \"negative\"])]\n",
        "\n",
        "# Map labels\n",
        "label_map = {\"negative\": 0, \"positive\": 1}\n",
        "df[\"label\"] = df[\"targets\"].map(label_map)\n"
      ],
      "metadata": {
        "id": "KN_WDOhdViMX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset into training and testing sets using sklearn\n"
      ],
      "metadata": {
        "id": "2LtTazUbeVGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"inputs\"], df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "1dc2MlNdVym3"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the text using DistilBERT tokenizer for input to the transformer model\n"
      ],
      "metadata": {
        "id": "7p82Pij7eX4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "QVuboS9WV05l"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "z7SpZcqLV7_M"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping tokenized inputs and labels in a PyTorch Dataset class for Trainer compatibility\n"
      ],
      "metadata": {
        "id": "jB40W25tec9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, list(train_labels))\n",
        "test_dataset = CustomDataset(test_encodings, list(test_labels))\n"
      ],
      "metadata": {
        "id": "leWrojv6WBYe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the base DistilBERT model with 2 output labels (positive and negative)\n"
      ],
      "metadata": {
        "id": "shw_WZL_WagR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hzLD5qWfpB",
        "outputId": "03aa1c57-f19c-46bc-8f57-5a2c066810c7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining alll the training arguments"
      ],
      "metadata": {
        "id": "cTNlvp7cWlnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I just realised the colab notebook is using the older version of the 'transformers' library so it doesn't support the evaluation_strategy keyword yet.\n",
        "so upgrading to the latest version"
      ],
      "metadata": {
        "id": "IPy2hpqiW6tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.41.2 --force-reinstall --upgrade --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu4VYlh1Wv6H",
        "outputId": "48d2b221-e656-4ae5-933a-d2ce034a256d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "langchain-core 0.3.65 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 fsspec==2025.3.0 requests==2.32.3 packaging==24.0 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laHNPyeWXNbj",
        "outputId": "4697ca3d-c065-4d9b-e66d-55593b6abefc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "google-cloud-bigquery 3.34.0 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./yelp_model\")\n",
        "tokenizer.save_pretrained(\"./yelp_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ60M9nsYCOh",
        "outputId": "ff362e3a-e090-40f8-e423-1b2a602dd29b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./yelp_model/tokenizer_config.json',\n",
              " './yelp_model/special_tokens_map.json',\n",
              " './yelp_model/vocab.txt',\n",
              " './yelp_model/added_tokens.json',\n",
              " './yelp_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xMAvNNZYz99",
        "outputId": "3db45942-493a-4d71-a560-d772b142902a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.9)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.65)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Node Functions\n",
        "We'll define Python functions for:\n",
        "\n",
        "Inference using your yelp_model\n",
        "\n",
        "Confidence threshold check (may be  < 70%)\n",
        "\n",
        "Fallback that asks user input if confidence is too low\n",
        "\n"
      ],
      "metadata": {
        "id": "qX0D1ZRrY9PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "interference nodes"
      ],
      "metadata": {
        "id": "9j__mTIWZU9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_path = \"./yelp_model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def inference_node(state):\n",
        "    text = state[\"input\"]\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=-1)\n",
        "    confidence, prediction = torch.max(probs, dim=1)\n",
        "\n",
        "    return {\n",
        "        \"input\": text,\n",
        "        \"prediction\": prediction.item(),\n",
        "        \"confidence\": confidence.item(),\n",
        "        \"probs\": probs.squeeze().tolist()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "c8KlXoM3Y4Z9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "confidence check node"
      ],
      "metadata": {
        "id": "ZtKsqZkoZbjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence_check_node(state, threshold=0.7):\n",
        "    if state[\"confidence\"] < threshold:\n",
        "        # Ask for clarification manually\n",
        "        print(f\"[Fallback Triggered] Confidence: {state['confidence']*100:.2f}%\")\n",
        "        user_input = input(\"Do you agree with the prediction? (yes/no): \").strip().lower()\n",
        "        if user_input == \"no\":\n",
        "            correct_label = int(input(\"Enter correct label (0 = Negative, 1 = Positive): \"))\n",
        "            state[\"final_label\"] = correct_label\n",
        "            state[\"corrected\"] = True\n",
        "        else:\n",
        "            state[\"final_label\"] = state[\"prediction\"]\n",
        "            state[\"corrected\"] = False\n",
        "    else:\n",
        "        state[\"final_label\"] = state[\"prediction\"]\n",
        "        state[\"corrected\"] = False\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "jXbn6DzwZZcA"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fall back node or more like a user interfere node or backup node"
      ],
      "metadata": {
        "id": "0LQXNqG3qunT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fallback_node(state):\n",
        "    print(f\"[FallbackNode] Low confidence ({state['confidence']*100:.1f}%).\")\n",
        "    print(f\"Model predicted: {'Positive' if state['prediction'] == 1 else 'Negative'}\")\n",
        "    user_input = input(\"Do you want to correct the prediction? (yes/no): \").strip().lower()\n",
        "\n",
        "    if user_input == \"yes\":\n",
        "        label = input(\"Enter correct label (0=Negative, 1=Positive): \").strip()\n",
        "        return {\n",
        "            **state,\n",
        "            \"corrected\": True,\n",
        "            \"final_label\": int(label)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            **state,\n",
        "            \"corrected\": False,\n",
        "            \"final_label\": state[\"prediction\"]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ZuzuhDHGZfnN"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we define the DAG flow!!"
      ],
      "metadata": {
        "id": "Le25LlukZrBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "builder = StateGraph(dict)\n",
        "\n",
        "builder.add_node(\"Inference\", inference_node)\n",
        "builder.add_node(\"CheckConfidence\", confidence_check_node)\n",
        "\n",
        "builder.set_entry_point(\"Inference\")\n",
        "builder.add_edge(\"Inference\", \"CheckConfidence\")\n",
        "builder.add_edge(\"CheckConfidence\", END)\n",
        "\n",
        "graph = builder.compile()\n"
      ],
      "metadata": {
        "id": "her6VOoiZtWT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLI Loop"
      ],
      "metadata": {
        "id": "cYMu1VlxaR97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def log_interaction(state):\n",
        "    with open(\"log.txt\", \"a\") as log_file:\n",
        "        log_file.write(f\"[{datetime.now()}]\\n\")\n",
        "        log_file.write(f\"Input: {state['input']}\\n\")\n",
        "        log_file.write(f\"Predicted: {'Positive' if state['prediction'] == 1 else 'Negative'} | Confidence: {state['confidence']*100:.2f}%\\n\")\n",
        "        log_file.write(f\"Fallback Triggered: {'Yes' if state['confidence'] < 0.7 else 'No'}\\n\")\n",
        "        log_file.write(f\"User Correction: {'Yes' if state.get('corrected') else 'No'}\\n\")\n",
        "        log_file.write(f\"Final Label: {'Positive' if state['final_label'] == 1 else 'Negative'}\\n\")\n",
        "        log_file.write(\"-\" * 50 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "XruyDMfVbAtH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PHMvBs6Io6b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLI Loop\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sentence (or 'exit' to quit): \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Initial state\n",
        "    state = {\"input\": user_input}\n",
        "\n",
        "    # Run through LangGraph DAG\n",
        "    final_state = graph.invoke(state)\n",
        "\n",
        "    # Log interaction\n",
        "    log_interaction(final_state)\n",
        "\n",
        "    # Display output\n",
        "    label = final_state[\"final_label\"]\n",
        "    print(f\"\\n Final Label: {'Positive' if label == 1 else 'Negative'}\")\n",
        "    print(f\" Confidence: {final_state['confidence']*100:.2f}%\")\n",
        "    print(f\" Corrected by user: {'Yes' if final_state['corrected'] else 'No'}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgBtfUL3aTao",
        "outputId": "75bfe269-90ec-431b-d03a-e644cf05e539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter a sentence (or 'exit' to quit): the food was great \n",
            "[Fallback Triggered] Confidence: 52.59%\n",
            "Do you agree with the prediction? (yes/no): yes\n",
            "\n",
            " Final Label: Negative\n",
            " Confidence: 52.59%\n",
            " Corrected by user: No\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# self-healing-sentiment-dag\n",
        "\n",
        "A CLI-based sentiment classifier with fallback logic using a LangGraph DAG and a fine-tuned DistilBERT model. Automatically requests user clarification when prediction confidence is low.\n",
        "\n",
        "---\n",
        "\n",
        "##  Project Title: Self-Healing Sentiment Classifier with Confidence-Aware LangGraph DAG\n",
        "\n",
        "This project implements a robust command-line interface (CLI) application that performs sentiment classification using a fine-tuned transformer model on the Yelp polarity dataset. The core of this system is a **LangGraph-based Directed Acyclic Graph (DAG)** that integrates self-healing mechanisms to ensure accurate and trustworthy predictionsâ€”particularly in cases where the model's confidence is low.\n",
        "\n",
        "---\n",
        "\n",
        "##  Demo Video\n",
        "\n",
        "Watch the full walkthrough of the CLI-based self-healing sentiment classifier in action:\n",
        "\n",
        "ðŸ”— [Watch on Loom](https://www.loom.com/share/2cf567bde2d646b59296d515891219e3?sid=8cd8f06a-9583-482c-ae9e-efe10f1d6e0f)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##  Overview\n",
        "\n",
        "- **Dataset Used**: Yelp Polarity (binary sentiment: positive/negative)\n",
        "- **Model**: DistilBERT (fine-tuned using Hugging Face + LoRA/full finetuning)\n",
        "- **Framework**: LangGraph for decision DAG\n",
        "- **Fallback**: Triggered if confidence < 70%\n",
        "- **Interface**: Fully interactive CLI with logging and correction capabilities\n",
        "\n",
        "---\n",
        "\n",
        "##  Features\n",
        "\n",
        "- Fine-tuned transformer for sentiment analysis\n",
        "- LangGraph DAG with:\n",
        "  - `InferenceNode`: Predicts sentiment\n",
        "  - `ConfidenceCheckNode`: Checks prediction confidence\n",
        "  - `FallbackNode`: Requests clarification if confidence is low\n",
        "- Self-healing logic with user input recovery\n",
        "- CLI-based interaction loop\n",
        "- Structured logging of all predictions, confidence scores, and corrections\n",
        "\n",
        "---\n",
        "\n",
        "##  How It Works\n",
        "\n",
        "### 1. Prediction Phase\n",
        "User inputs a sentence. The model predicts its sentiment and provides a confidence score.\n",
        "\n",
        "### 2. Confidence Check\n",
        "If confidence < 70%, a fallback is triggered.\n",
        "\n",
        "### 3. Fallback and Clarification\n",
        "The system prompts the user to clarify or confirm their intent. The final label is then logged.\n",
        "\n",
        "### Example CLI Output:\n",
        "Input: The movie was painfully slow and boring.\n",
        "\n",
        "[InferenceNode] Predicted label: Positive | Confidence: 54%\n",
        "\n",
        "[ConfidenceCheckNode] Confidence too low. Triggering fallback...\n",
        "\n",
        "[FallbackNode] Could you clarify your intent? Was this a negative review?\n",
        "\n",
        "User: Yes, it was definitely negative.\n",
        "\n",
        "Final Label: Negative (Corrected via user clarification)\n",
        "\n",
        "---\n",
        "\n",
        "##  Setup Instructions\n",
        "\n",
        "### 1. Clone the Repository\n",
        "```bash\n",
        "git clone https://github.com/yourusername/self-healing-sentiment-dag.git\n",
        "cd self-healing-sentiment-dag\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "0uKmlkFkb5No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_content = \"\"\"\n",
        "[2025-06-25 01:23:45]\n",
        "Input: The food was cold\n",
        "Predicted: Positive | Confidence: 54.32%\n",
        "Fallback Triggered: Yes\n",
        "User Correction: Yes\n",
        "Final Label: Negative\n",
        "--------------------------------------------------\n",
        "\n",
        "[2025-06-25 01:25:01]\n",
        "Input: Amazing service and cozy atmosphere\n",
        "Predicted: Positive | Confidence: 89.47%\n",
        "Fallback Triggered: No\n",
        "User Correction: No\n",
        "Final Label: Positive\n",
        "--------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "with open(\"log.txt\", \"w\") as f:\n",
        "    f.write(log_content.strip())\n"
      ],
      "metadata": {
        "id": "j7FzriSYxa_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHYja2B30ecZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}