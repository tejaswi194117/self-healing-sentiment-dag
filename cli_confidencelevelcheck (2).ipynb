{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d33ca58bbb6649f4b1441adc88179804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cb85b46fea249d590bcce56bf867e3b",
              "IPY_MODEL_05b78bde6da945be85c9106377a8746b",
              "IPY_MODEL_5207d0c1a402409c8a774c01ca1bf76b"
            ],
            "layout": "IPY_MODEL_42bbab17614746b5b7aa1d032c579f7a"
          }
        },
        "2cb85b46fea249d590bcce56bf867e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_602ac1d68d394504a909c583f8ea24ce",
            "placeholder": "​",
            "style": "IPY_MODEL_004696d2d6b640c0af6a4fbefff202a4",
            "value": ""
          }
        },
        "05b78bde6da945be85c9106377a8746b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61666fee7ba74c06b075752c74390b37",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e69f31deb29b4577be2b63342e0dc340",
            "value": 0
          }
        },
        "5207d0c1a402409c8a774c01ca1bf76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cbf8d4de01b45dab9363b0161ab0441",
            "placeholder": "​",
            "style": "IPY_MODEL_45be16cc28394edb88943bde70cdfe13",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "42bbab17614746b5b7aa1d032c579f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "602ac1d68d394504a909c583f8ea24ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004696d2d6b640c0af6a4fbefff202a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61666fee7ba74c06b075752c74390b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e69f31deb29b4577be2b63342e0dc340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cbf8d4de01b45dab9363b0161ab0441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45be16cc28394edb88943bde70cdfe13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing all required libraries: Transformers, Datasets, Accelerate, LangGraph, and Torch\n"
      ],
      "metadata": {
        "id": "aVpW7ZsiRzwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers huggingface_hub --quiet\n"
      ],
      "metadata": {
        "id": "gQbqNMBeR5cY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Loading the Dataset (Post Installation)\n",
        "\n",
        "Initially, I attempted to load the dataset using `load_dataset(\"imdb\")` right after installing the required libraries.\n",
        "\n",
        "However, I noticed that no output or data was loading correctly. After analysis, I realized the error stemmed from unresolved file system paths — just mentioning `\"imdb\"` wasn't enough. Using the full dataset path was supposed to solve this issue and avoid unexpected directory-related bugs.\n",
        "\n",
        "Even after adjusting the paths, I discovered that the IMDb dataset itself was causing errors due to a known compatibility issue between `datasets` and `fsspec`.\n",
        "\n",
        "So I decided to switch to a similar open-access dataset that is `yelp_polarity`, which also performs binary sentiment classification. Unfortunately, it too had download issues due to the same loader bug.\n",
        "\n",
        "\n",
        "\n",
        "So I switched to Plan B:\n",
        "\n",
        "I decided to \"manually download a sentiment dataset in CSV format\" and use it directly with pandas — this gave me full control, avoided dependency bugs, and allowed me to continue fine-tuning the model without relying on unstable APIs.\n",
        "\n",
        "I used this approach because it aligned with the task's requirements for using an open-access classification dataset while ensuring my training pipeline remained functional and reproducible.\n"
      ],
      "metadata": {
        "id": "4s5VJs9GR8sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the specific file (file path might differ; adjust accordingly)\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"supergoose/flan_combined_yelp_polarity_reviews_0_2_0\",\n",
        "    filename=\"data/train-00000-of-00001.parquet\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "print(\"Downloaded to:\", file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkU-SNQmSA5K",
        "outputId": "6b2900d8-7cde-45ed-889e-8d8784f3b70e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded to: /root/.cache/huggingface/hub/datasets--supergoose--flan_combined_yelp_polarity_reviews_0_2_0/snapshots/1c8123a3698002300c0a46dc39824f61110a440c/data/train-00000-of-00001.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the parquet file into a DataFrame and selecting only relevant columns\n"
      ],
      "metadata": {
        "id": "aDwcduoZSSlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(file_path)\n",
        "print(df.head(), df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-RdoV2vSXqQ",
        "outputId": "4f1857a2-4e19-48f0-f918-517d120cdb2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              inputs  \\\n",
            "0  input: Write a negative yelp review.\\noutput: ...   \n",
            "1  Sentiment analysis: Wasn't in the mood for a s...   \n",
            "2  Input:  What would be an example of an positiv...   \n",
            "3  Problem: I'm not a major fan of their coffee r...   \n",
            "4  Problem: Visited again today as I was famished...   \n",
            "\n",
            "                                             targets  _template_idx  \\\n",
            "0  Went to Henderson location it's ok, another sp...              6   \n",
            "1                                           positive              8   \n",
            "2  An example of an positive review: LOVE me some...              4   \n",
            "3                                           positive              9   \n",
            "4                                           negative              9   \n",
            "\n",
            "  _task_source                   _task_name _template_type  \n",
            "0     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "1     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "2     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "3     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt  \n",
            "4     Flan2021  yelp_polarity_reviews:0.2.0       fs_noopt   (41202, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting into train and test"
      ],
      "metadata": {
        "id": "K2d31NoGVeYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLTbYPT2VhTh",
        "outputId": "c73213a4-c451-4b0a-f027-541601a55ac1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['inputs', 'targets', '_template_idx', '_task_source', '_task_name',\n",
              "       '_template_type'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering only positive/negative samples and mapping labels to integers (0/1)\n"
      ],
      "metadata": {
        "id": "-q4BTTL7eNiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"inputs\", \"targets\"]]\n",
        "df = df[df[\"targets\"].isin([\"positive\", \"negative\"])]\n",
        "\n",
        "# Map labels\n",
        "label_map = {\"negative\": 0, \"positive\": 1}\n",
        "df[\"label\"] = df[\"targets\"].map(label_map)\n"
      ],
      "metadata": {
        "id": "KN_WDOhdViMX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset into training and testing sets using sklearn\n"
      ],
      "metadata": {
        "id": "2LtTazUbeVGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"inputs\"], df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "1dc2MlNdVym3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the text using DistilBERT tokenizer for input to the transformer model\n"
      ],
      "metadata": {
        "id": "7p82Pij7eX4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "d33ca58bbb6649f4b1441adc88179804",
            "2cb85b46fea249d590bcce56bf867e3b",
            "05b78bde6da945be85c9106377a8746b",
            "5207d0c1a402409c8a774c01ca1bf76b",
            "42bbab17614746b5b7aa1d032c579f7a",
            "602ac1d68d394504a909c583f8ea24ce",
            "004696d2d6b640c0af6a4fbefff202a4",
            "61666fee7ba74c06b075752c74390b37",
            "e69f31deb29b4577be2b63342e0dc340",
            "9cbf8d4de01b45dab9363b0161ab0441",
            "45be16cc28394edb88943bde70cdfe13"
          ]
        },
        "id": "QVuboS9WV05l",
        "outputId": "5c5eaf44-8797-48ba-971e-5506cc86e844"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33ca58bbb6649f4b1441adc88179804"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "z7SpZcqLV7_M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping tokenized inputs and labels in a PyTorch Dataset class for Trainer compatibility\n"
      ],
      "metadata": {
        "id": "jB40W25tec9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, list(train_labels))\n",
        "test_dataset = CustomDataset(test_encodings, list(test_labels))\n"
      ],
      "metadata": {
        "id": "leWrojv6WBYe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the base DistilBERT model with 2 output labels (positive and negative)\n"
      ],
      "metadata": {
        "id": "shw_WZL_WagR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hzLD5qWfpB",
        "outputId": "43356a28-5677-4216-e37d-c68aa55be844"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining alll the training arguments"
      ],
      "metadata": {
        "id": "cTNlvp7cWlnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I just realised the colab notebook is using the older version of the 'transformers' library so it doesn't support the evaluation_strategy keyword yet.\n",
        "so upgrading to the latest version"
      ],
      "metadata": {
        "id": "IPy2hpqiW6tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.41.2 --force-reinstall --upgrade --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu4VYlh1Wv6H",
        "outputId": "e5f6b672-ba01-49f5-d2e1-127c98299933"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.7/157.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "langchain-core 0.3.65 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 fsspec==2025.3.0 requests==2.32.3 packaging==24.0 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laHNPyeWXNbj",
        "outputId": "10940d7f-5a33-4183-f219-823d11eb7012"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "google-cloud-bigquery 3.34.0 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./yelp_model\")\n",
        "tokenizer.save_pretrained(\"./yelp_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ60M9nsYCOh",
        "outputId": "8e23033d-8644-47e1-83ff-361ee0e5b42a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./yelp_model/tokenizer_config.json',\n",
              " './yelp_model/special_tokens_map.json',\n",
              " './yelp_model/vocab.txt',\n",
              " './yelp_model/added_tokens.json',\n",
              " './yelp_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xMAvNNZYz99",
        "outputId": "0428fa9e-e5ba-47f3-850f-a8e3d2dce086"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.65)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.4.8 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qP9lNjmEek7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Node Functions\n",
        "We'll define Python functions for:\n",
        "\n",
        "Inference using your yelp_model\n",
        "\n",
        "Confidence threshold check (may be  < 70%)\n",
        "\n",
        "Fallback that asks user input if confidence is too low\n",
        "\n"
      ],
      "metadata": {
        "id": "qX0D1ZRrY9PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "interference nodes"
      ],
      "metadata": {
        "id": "9j__mTIWZU9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_path = \"./yelp_model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def inference_node(state):\n",
        "    text = state[\"input\"]\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=-1)\n",
        "    confidence, prediction = torch.max(probs, dim=1)\n",
        "\n",
        "    return {\n",
        "        \"input\": text,\n",
        "        \"prediction\": prediction.item(),\n",
        "        \"confidence\": confidence.item(),\n",
        "        \"probs\": probs.squeeze().tolist()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "c8KlXoM3Y4Z9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "confidence check node"
      ],
      "metadata": {
        "id": "ZtKsqZkoZbjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence_check_node(state, threshold=0.7):\n",
        "    if state[\"confidence\"] < threshold:\n",
        "        # Ask for clarification manually\n",
        "        print(f\"[Fallback Triggered] Confidence: {state['confidence']*100:.2f}%\")\n",
        "        user_input = input(\"Do you agree with the prediction? (yes/no): \").strip().lower()\n",
        "        if user_input == \"no\":\n",
        "            correct_label = int(input(\"Enter correct label (0 = Negative, 1 = Positive): \"))\n",
        "            state[\"final_label\"] = correct_label\n",
        "            state[\"corrected\"] = True\n",
        "        else:\n",
        "            state[\"final_label\"] = state[\"prediction\"]\n",
        "            state[\"corrected\"] = False\n",
        "    else:\n",
        "        state[\"final_label\"] = state[\"prediction\"]\n",
        "        state[\"corrected\"] = False\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "jXbn6DzwZZcA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fall back node or more like a user interfere node or backuop node"
      ],
      "metadata": {
        "id": "cdiebsKfZibP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fallback_node(state):\n",
        "    print(f\"[FallbackNode] Low confidence ({state['confidence']*100:.1f}%).\")\n",
        "    print(f\"Model predicted: {'Positive' if state['prediction'] == 1 else 'Negative'}\")\n",
        "    user_input = input(\"Do you want to correct the prediction? (yes/no): \").strip().lower()\n",
        "\n",
        "    if user_input == \"yes\":\n",
        "        label = input(\"Enter correct label (0=Negative, 1=Positive): \").strip()\n",
        "        return {\n",
        "            **state,\n",
        "            \"corrected\": True,\n",
        "            \"final_label\": int(label)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            **state,\n",
        "            \"corrected\": False,\n",
        "            \"final_label\": state[\"prediction\"]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ZuzuhDHGZfnN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we define the DAG flow!!"
      ],
      "metadata": {
        "id": "Le25LlukZrBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "builder = StateGraph(dict)\n",
        "\n",
        "builder.add_node(\"Inference\", inference_node)\n",
        "builder.add_node(\"CheckConfidence\", confidence_check_node)\n",
        "\n",
        "builder.set_entry_point(\"Inference\")\n",
        "builder.add_edge(\"Inference\", \"CheckConfidence\")\n",
        "builder.add_edge(\"CheckConfidence\", END)\n",
        "\n",
        "graph = builder.compile()\n"
      ],
      "metadata": {
        "id": "her6VOoiZtWT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLI Loop"
      ],
      "metadata": {
        "id": "cYMu1VlxaR97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def log_interaction(state):\n",
        "    with open(\"log.txt\", \"a\") as log_file:\n",
        "        log_file.write(f\"[{datetime.now()}]\\n\")\n",
        "        log_file.write(f\"Input: {state['input']}\\n\")\n",
        "        log_file.write(f\"Predicted: {'Positive' if state['prediction'] == 1 else 'Negative'} | Confidence: {state['confidence']*100:.2f}%\\n\")\n",
        "        log_file.write(f\"Fallback Triggered: {'Yes' if state['confidence'] < 0.7 else 'No'}\\n\")\n",
        "        log_file.write(f\"User Correction: {'Yes' if state.get('corrected') else 'No'}\\n\")\n",
        "        log_file.write(f\"Final Label: {'Positive' if state['final_label'] == 1 else 'Negative'}\\n\")\n",
        "        log_file.write(\"-\" * 50 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "XruyDMfVbAtH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLI Loop\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sentence (or 'exit' to quit): \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Initial state\n",
        "    state = {\"input\": user_input}\n",
        "\n",
        "    # Run through LangGraph DAG\n",
        "    final_state = graph.invoke(state)\n",
        "\n",
        "    # Log interaction\n",
        "    log_interaction(final_state)\n",
        "\n",
        "    # Display output\n",
        "    label = final_state[\"final_label\"]\n",
        "    print(f\"\\n Final Label: {'Positive' if label == 1 else 'Negative'}\")\n",
        "    print(f\" Confidence: {final_state['confidence']*100:.2f}%\")\n",
        "    print(f\" Corrected by user: {'Yes' if final_state['corrected'] else 'No'}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgBtfUL3aTao",
        "outputId": "cc89142e-8780-416b-c06f-c66eb232d683"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter a sentence (or 'exit' to quit): the food was cool and awful \n",
            "[Fallback Triggered] Confidence: 52.55%\n",
            "Do you agree with the prediction? (yes/no): no\n",
            "Enter correct label (0 = Negative, 1 = Positive): 0\n",
            "\n",
            " Final Label: Negative\n",
            " Confidence: 52.55%\n",
            " Corrected by user: Yes\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter a sentence (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readme_content = \"\"\"#  Self-Healing Sentiment Classifier using LangGraph & Fine-Tuned DistilBERT\n",
        "\n",
        "This project builds a CLI-based sentiment classifier that can correct itself using fallback logic when the model is unsure.\n",
        "\n",
        "---\n",
        "\n",
        "##  Project Goals\n",
        "\n",
        "- Fine-tune a DistilBERT model on Yelp polarity data.\n",
        "- Create a LangGraph DAG with:\n",
        "  - Inference node\n",
        "  - Confidence check node\n",
        "  - Fallback interaction node\n",
        "- Automatically log inputs, predictions, fallback use, and final labels.\n",
        "\n",
        "---\n",
        "\n",
        "##  Setup\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets accelerate langgraph torch#\"\"\"\n"
      ],
      "metadata": {
        "id": "0uKmlkFkb5No"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ub1HCoQefJe1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}